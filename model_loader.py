from sentence_transformers import SentenceTransformer, models
import os
import torch

class EmbeddingModel:
    def __init__(self):
        # 1. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ (å…³é”®ä¿®æ”¹)
        # å¦‚æœè£…äº† GPU ç‰ˆ torchï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å˜æˆ 'cuda'
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ Using Device: {self.device.upper()}") 

        base_path = os.path.dirname(os.path.abspath(__file__))
        text_model_path = os.path.join(base_path, "models/all-MiniLM-L6-v2")
        clip_model_path = os.path.join(base_path, "models/clip-ViT-B-32")

        # 2. åŠ è½½æ–‡æœ¬æ¨¡å‹ (ä¼ å…¥ device å‚æ•°)
        print(f"Loading Text Model from: {text_model_path} ...")
        self.text_model = SentenceTransformer(text_model_path, device=self.device)

        # 3. åŠ è½½ CLIP æ¨¡å‹ (ä¼ å…¥ device å‚æ•°)
        print(f"Loading CLIP Model from: {clip_model_path} ...")
        try:
            # æ˜¾å¼åŠ è½½ CLIP æ¨¡å—
            clip_module = models.CLIPModel(clip_model_path)
            self.clip_model = SentenceTransformer(modules=[clip_module], device=self.device)
        except Exception as e:
            print(f"æ ‡å‡†åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {e}")
            self.clip_model = SentenceTransformer(clip_model_path, device=self.device)

    def get_text_embedding(self, text):
        return self.text_model.encode(text).tolist()

    def get_image_embedding(self, image):
        return self.clip_model.encode(image).tolist()
    
    def get_text_for_image_embedding(self, text):
        return self.clip_model.encode(text).tolist()